{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify the type of classifer you want to use: svm/naive/max_ent/neural? svm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-bd72f253d5d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m# LinearSvc with hash vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mNB_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashTrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0mtest_predict_NB_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNB_hash\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/lovelace/software/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    585\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[1;32m    586\u001b[0m                                        dtype=np.float64)\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_class_log_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/lovelace/software/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from analogy_strings import analogy_string_list\n",
    "from sentence_parser import get_speech_tags\n",
    "from personal import root\n",
    "\n",
    "#------------------------\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from nltk.classify import maxent\n",
    "import sys\n",
    "#------------------------\n",
    "import random\n",
    "import re\n",
    "from boyer_moore import find_boyer_moore\n",
    "\n",
    "# Implementation of the classifier to detect analogy.\n",
    "# From http://www.nltk.org/book/ch06.html\n",
    "\n",
    "analogy_file_name = \"test_extractions/bc_analogy.txt\"\n",
    "non_analogy_file_name = \"test_extractions/bc_ground.txt\"\n",
    "\n",
    "def get_analogy_string(text, mode):\n",
    "    # Returns a tuple of the first analogy indicator along with its speech tag in the text.\n",
    "    # e.g. (('as', 'RB'), ('fast', 'RB'), ('as', 'IN'))\n",
    "    result = []\n",
    "    tokens = text.split()\n",
    "    tagged_text = nltk.pos_tag(tokens)\n",
    "\n",
    "    for pattern in analogy_string_list:\n",
    "        match_index = find_boyer_moore(tokens, pattern)\n",
    "        if match_index != -1:\n",
    "            for i in range(len(pattern)):\n",
    "                result.append(tagged_text[match_index + i])\n",
    "            if (mode == \"naive\" or mode == \"max_ent\"):\n",
    "                return {\"analogy_indicator:\" : tuple(result)}\n",
    "            elif (mode == \"svm\"):\n",
    "                return (result)\n",
    "            \n",
    "    if (mode == \"naive\" or mode == \"max_ent\"):\n",
    "        return {\"analogy_indicator:\" : tuple(result)}\n",
    "    elif (mode == \"svm\"):\n",
    "        return (result)\n",
    "              \n",
    "\n",
    "def get_list(filename):\n",
    "    # Returns all training data as a list\n",
    "    # File should be formatted as a text line followed '>' in the next line\n",
    "    # before a new text line.\n",
    "    list = []\n",
    "    file = open(filename, \"r\", encoding = \"utf-8\")\n",
    "    for line in file.readlines():\n",
    "        if line[0] != '>' and line != \"\\n\":\n",
    "            final = line.split(\"]\\\",\")[-1].split(\".\")[0].split(\"?\")[0].split(\"!\")[0]\n",
    "            list.append(final)\n",
    "\n",
    "    return list\n",
    "\n",
    "def get_list_re(filename):\n",
    "    # Returns all training data as a list\n",
    "    # File should be formatted as a text line followed '>' in the next line\n",
    "    # before a new text line.\n",
    "    list = []\n",
    "    file = open(filename, \"r\", encoding = \"utf-8\")\n",
    "    for line in file.readlines():\n",
    "        if line[0] != '>' and line != \"\\n\":\n",
    "            l = line.split(\"]\\\",\")[-1]\n",
    "            final = re.sub(\"[^a-zA-Z]\",\" \", l)\n",
    "            #final = line.split(\"]\\\",\")[-1].split(\".\")[0].split(\"?\")[0].split(\"!\")[0]\n",
    "            list.append(final)\n",
    "\n",
    "    return list\n",
    "\n",
    "analogy_list = get_list_re(analogy_file_name)\n",
    "non_analogy_list = get_list_re(non_analogy_file_name)\n",
    "\n",
    "#print(get_list_re(\"test_extractions/prove.txt\"))\n",
    "#print(analogy_list[157])\n",
    "\n",
    "# labeled data.\n",
    "samples = [(text, 'YES') for text in analogy_list] + [(text, 'NO') for text in non_analogy_list]\n",
    "mid_point = int(len(samples) / 2)\n",
    "random.shuffle(samples)\n",
    "\n",
    "# verify that the classifier is implemented\n",
    "mode = input(\"Please specify the type of classifer you want to use: svm/naive/max_ent/neural? \")\n",
    "if (mode != \"svm\") and (mode != \"naive\") and (mode != \"max_ent\") and (mode != \"neural\"):\n",
    "    sys.exit(\"This classifier has not been implemented yet.\")\n",
    "\n",
    "# divide data into training set and test set\n",
    "#feature_sets = [(get_analogy_string(text, mode), label) for (text, label) in samples]\n",
    "#feature_sets = [(text, label) for (text, label) in samples]\n",
    "#mid_point = int(len(samples) / 2)\n",
    "#train_set =  feature_sets[: mid_point]\n",
    "#test_set = feature_sets[mid_point :]\n",
    "\n",
    "\n",
    "if mode == \"svm\":\n",
    "    # Preparing the data\n",
    "    feature_sets = [(text, label) for (text, label) in samples]\n",
    "    train_set =  feature_sets[: mid_point]\n",
    "    test_set = feature_sets[mid_point :]\n",
    "    train_data = [text for (text, label) in train_set]\n",
    "    train_labels = [label for (text, label) in train_set]\n",
    "    test_data = [text for (text, label) in test_set]\n",
    "    test_labels = [label for (text, label) in test_set]\n",
    "    # Transforming the data using tf-idf\n",
    "    TfidfVect = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    TfidfTrans = TfidfVect.fit_transform(train_data)\n",
    "    TfidfTrans_test = TfidfVect.transform(test_data)\n",
    "    # Transforming the data using count vectorizer\n",
    "    CountVect = CountVectorizer(lowercase=False)\n",
    "    CountTrans = CountVect.fit_transform(train_data)\n",
    "    CountTest = CountVect.transform(test_data)\n",
    "    # Transforming the data using hashing vectorizer\n",
    "    HashVect = HashingVectorizer(lowercase=False)\n",
    "    HashTrans = HashVect.fit_transform(train_data)\n",
    "    HashTest = HashVect.transform(test_data)\n",
    "    # LinearSVC with tf-idf\n",
    "    LinearSvc = LinearSVC().fit(TfidfTrans, train_labels)\n",
    "    test_predict_LinearSvc_tf = LinearSvc.predict(TfidfTrans_test)\n",
    "    \n",
    "    # NB with count vectorizer\n",
    "    NB_count  = MultinomialNB().fit(CountTrans, train_labels)\n",
    "    test_predict_NB_count = NB_count.predict(CountTest)\n",
    "    \n",
    "   \n",
    "    # SVC with td-idf\n",
    "    NB_tf = MultinomialNB().fit(TfidfTrans, train_labels)\n",
    "    test_predict_NB_tf = NB.predict(TfidfTrans_test)\n",
    "    \n",
    "    print(\"NB with count vec: \", NB_count.score(CountTest, test_labels))\n",
    "    print(\"Confusion matrix for SVC with count :\\n\", confusion_matrix(test_labels,test_predict_NB_count,labels=[\"YES\", \"NO\"]))\n",
    "   print(\"NB with hash vec: \", NB_hash.score(HashTest, test_labels))\n",
    "    print(\"Confusion matrix for NB with hash :\\n\", confusion_matrix(test_labels,test_predict_NB_hash,labels=[\"YES\", \"NO\"]))\n",
    "    print()\n",
    "    \n",
    "    LinearSvc_count = LinearSVC().fit(CountTrans, train_labels)\n",
    "    test_predict_LinearSvc_count = LinearSvc_count.predict(CountTest)\n",
    "    # LinearSvc with hash vectorizer\n",
    "    LinearSvc_hash = LinearSVC().fit(HashTrans, train_labels)\n",
    "    test_predict_LinearSvc_hash = LinearSvc_hash.predict(HashTest)\n",
    "    # SVC with td-idf\n",
    "    Svc_tf = SVC().fit(TfidfTrans, train_labels)\n",
    "    test_predict_Svc_tf = Svc_tf.predict(TfidfTrans_test)\n",
    "    # SVC with count vectorizer\n",
    "    Svc_count = SVC().fit(CountTrans, train_labels)\n",
    "    test_predict_Svc_count = Svc_count.predict(CountTest)\n",
    "    # SVC with hash vectorizer\n",
    "    Svc_hash = SVC().fit(HashTrans, train_labels)\n",
    "    test_predict_Svc_hash = Svc_hash.predict(HashTest)\n",
    "    # NuSVC with tf-idf\n",
    "    NuSvc_tf = NuSVC().fit(TfidfTrans, train_labels)\n",
    "    test_predict_NuSVC_tf = NuSvc_tf.predict(TfidfTrans_test)\n",
    "    # NuSVC with count\n",
    "    NuSvc_count = NuSVC().fit(CountTrans, train_labels)\n",
    "    test_predict_NuSVC_count = NuSvc_count.predict(CountTest)\n",
    "    # NuSVC with hash vectorizer\n",
    "    NuSvc_hash = NuSVC().fit(HashTrans, train_labels)\n",
    "    test_predict_NuSVC_hash = NuSvc_hash.predict(HashTest)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Prints\n",
    "    print(\"LinearSvc with tf-idf: \", LinearSvc.score(TfidfTrans_test, test_labels))\n",
    "    print(\"Confusion matrix for LinearSVC with tf-idf :\\n\", confusion_matrix(test_labels,test_predict_LinearSvc_tf,labels=[\"YES\", \"NO\"]))\n",
    "    print(\"LinearSvc with count vec: \", LinearSvc_count.score(CountTest, test_labels))\n",
    "    print(\"Confusion matrix for LinearSVC with count :\\n\", confusion_matrix(test_labels,test_predict_LinearSvc_count,labels=[\"YES\", \"NO\"]))\n",
    "    print(\"LinearSvc with hash vec: \", LinearSvc_hash.score(HashTest, test_labels))\n",
    "    print(\"Confusion matrix for LinearSVC with hash :\\n\", confusion_matrix(test_labels,test_predict_LinearSvc_hash,labels=[\"YES\", \"NO\"]))\n",
    "    \n",
    "    print()\n",
    "    print(\"Svc with tf-idf: \", Svc_tf.score(TfidfTrans_test, test_labels))\n",
    "    print(\"Confusion matrix for SVC with tf-idf :\\n\", confusion_matrix(test_labels,test_predict_Svc_tf,labels=[\"YES\", \"NO\"]))\n",
    "    print(\"Svc with count: \", Svc_count.score(CountTest, test_labels))\n",
    "    print(\"Confusion matrix for SVC with count :\\n\", confusion_matrix(test_labels,test_predict_Svc_count,labels=[\"YES\", \"NO\"]))\n",
    "    print(\"Svc with hash: \", Svc_hash.score(HashTest, test_labels))\n",
    "    print(\"Confusion matrix for SVC with hash :\\n\", confusion_matrix(test_labels,test_predict_Svc_hash,labels=[\"YES\", \"NO\"]))\n",
    "        \n",
    "    print()\n",
    "    print(\"NuSvc with tf-idf: \", NuSvc_tf.score(TfidfTrans_test, test_labels))\n",
    "    print(\"Confusion matrix for NuSVC with tf-idf :\\n\", confusion_matrix(test_labels,test_predict_NuSVC_tf,labels=[\"YES\", \"NO\"]))\n",
    "    print(\"NuSvc with count: \", NuSvc_count.score(CountTest, test_labels))\n",
    "    print(\"Confusion matrix for NuSVC with count :\\n\", confusion_matrix(test_labels,test_predict_NuSVC_count,labels=[\"YES\", \"NO\"]))\n",
    "    print(\"NuSvc with hash: \", NuSvc_hash.score(HashTest, test_labels))\n",
    "    print(\"Confusion matrix for NuSVC with hash :\\n\", confusion_matrix(test_labels,test_predict_NuSVC_hash,labels=[\"YES\", \"NO\"]))\n",
    "               \n",
    "elif mode == \"max_ent\":\n",
    "    # preparing the data\n",
    "    feature_sets = [({\"text:\" : text}, label) for (text, label) in samples]\n",
    "    train_set =  feature_sets[: mid_point]\n",
    "    test_set = feature_sets[mid_point :]\n",
    "    max_ent = nltk.classify.MaxentClassifier.train(train_set, 'GIS', trace=0, max_iter=1000)\n",
    "    print(\"Maximum Entropy: nltk.classify.accuracy(max_ent, test_set)\")\n",
    "    #print(max_ent.show_most_informative_features(5))\n",
    "\n",
    "elif mode == \"neural\":\n",
    "    # Preparing the data\n",
    "    feature_sets = [(text, label) for (text, label) in samples]\n",
    "    train_set =  feature_sets[: mid_point]\n",
    "    test_set = feature_sets[mid_point :]\n",
    "    train_data = [text for (text, label) in train_set]\n",
    "    train_labels = [label for (text, label) in train_set]\n",
    "    test_data = [text for (text, label) in test_set]\n",
    "    test_labels = [label for (text, label) in test_set]\n",
    "    # Transforming the data using tf-idf\n",
    "    TfidfVect = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    TfidfTrans = TfidfVect.fit_transform(train_data)\n",
    "    TfidfTrans_test = TfidfVect.transform(test_data)\n",
    "    # Transforming the data using count vectorizer\n",
    "    CountVect = CountVectorizer(lowercase=False)\n",
    "    CountTrans = CountVect.fit_transform(train_data)\n",
    "    CountTest = CountVect.transform(test_data)\n",
    "    # Transforming the data using hashing vectorizer\n",
    "    HashVect = HashingVectorizer(lowercase=False)\n",
    "    HashTrans = HashVect.fit_transform(train_data)\n",
    "    HashTest = HashVect.transform(test_data)\n",
    "    # Neural Networks with tf-idf\n",
    "    MLP_tf = MLPClassifier().fit(TfidfTrans, train_labels)\n",
    "    test_predict_MLP_tf = MLP_tf.predict(TfidfTrans_test)\n",
    "    print(\"Neural Networks using MLP with tf-idf: \", MLP_tf.score(TfidfTrans_test, test_labels))\n",
    "    print(\"Confusion matrix for MLP with tf-idf :\\n\", confusion_matrix(test_labels,test_predict_MLP_tf,labels=[\"YES\", \"NO\"]))\n",
    "    # Neural Networks with count\n",
    "    MLP_count = MLPClassifier().fit(CountTrans, train_labels)\n",
    "    test_predict_MLP_count = MLP_count.predict(CountTest)\n",
    "    print(\"Neural Networks using MLP with count: \", MLP_count.score(CountTest, test_labels))\n",
    "    print(\"Confusion matrix for MLP with count :\\n\", confusion_matrix(test_labels,test_predict_MLP_count,labels=[\"YES\", \"NO\"]))\n",
    "    # Neural Networks with hash\n",
    "    MLP_hash = MLPClassifier().fit(HashTrans, train_labels)\n",
    "    test_predict_MLP_hash = MLP_hash.predict(HashTest)\n",
    "    print(\"Neural Networks using MLP with hash: \", MLP_hash.score(HashTest, test_labels))\n",
    "    print(\"Confusion matrix for MLP with hash :\\n\", confusion_matrix(test_labels,test_predict_MLP_hash,labels=[\"YES\", \"NO\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
