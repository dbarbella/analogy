{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lovelace/software/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2796 unique tokens\n",
      "Shape of data  (316, 28)\n",
      "Shape of label (316,)\n",
      "Shape of train_data (276, 28)\n",
      "Shape of test_data (40, 28)\n",
      "Shape of train_targets (276,)\n",
      "Shape of test_targets (40,)\n",
      "Found 400000 words\n",
      "processing fold: # 0\n",
      "processing fold: # 1\n",
      "processing fold: # 2\n",
      "processing fold: # 3\n",
      "40/40 [==============================] - 0s 335us/step\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "#phi - simple LSTM\n",
    "\n",
    "\n",
    "import csv\n",
    "def readCSV(fileName,r):\n",
    "    sent = []\n",
    "    with open(fileName) as file:\n",
    "        readcsv = csv.reader(file, delimiter=',')\n",
    "        for row in readcsv:\n",
    "            sentence = row[r]\n",
    "            sent.append(sentence)\n",
    "    return sent\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "neg = readCSV('../../corpora/verified_non_analogies.csv',1)\n",
    "pos = readCSV('../../corpora/verified_analogies.csv',1)\n",
    "labels = [1]*len(pos) + [0] * len(neg)\n",
    "texts = pos + neg\n",
    "\n",
    "# In[66]:\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 28 #maximum allowed number of words in a sentence \n",
    "\n",
    "max_words = 10000#choosing the most 10000 common words\n",
    "test = 40 #number of testing samples\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found {} unique tokens\".format(len(word_index)))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen = maxlen)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "train_data = data[:len(data)-test]\n",
    "test_data = data[len(data)-test:]\n",
    "train_targets = labels[:len(data) - test]\n",
    "test_targets = labels[len(data)-test:]\n",
    "\n",
    "print(\"Shape of data \", data.shape)\n",
    "print(\"Shape of label\", labels.shape)\n",
    "print(\"Shape of train_data\", train_data.shape)\n",
    "print(\"Shape of test_data\", test_data.shape)\n",
    "print(\"Shape of train_targets\", train_targets.shape)\n",
    "print(\"Shape of test_targets\", test_targets.shape)\n",
    "\n",
    "embedding_index = {}\n",
    "\n",
    "#download glove before this\n",
    "f = open(\"../../glove/glove.6B.100d.txt\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(\"Found {} words\".format(len(embedding_index)))\n",
    "\n",
    "import numpy as np\n",
    "k = 4 # k-fold\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "# Embed sentences\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, LSTM\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.layers[0].set_weights([embedding_matrix])\n",
    "    model.layers[0].trainable = False\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc', precision, recall])\n",
    "    model.save_weights('pre_trained_glove_model.h5')\n",
    "    return model\n",
    "\n",
    "#k-fold training\n",
    "val_acc_history = []\n",
    "acc_history = []\n",
    "for i in range(k):\n",
    "    print('processing fold: #',i)\n",
    "    val_data = train_data[i * num_val_samples: (i+1)*num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples : (i+1)*num_val_samples]\n",
    "    \n",
    "    partial_train_data = np.concatenate(\n",
    "    [train_data[:i*num_val_samples],\n",
    "    train_data[(i+1)*num_val_samples:]], axis = 0)\n",
    "    \n",
    "    partial_test_data = np.concatenate(\n",
    "    [train_targets[:i*num_val_samples],\n",
    "    train_targets[(i+1)*num_val_samples:]], axis = 0)\n",
    "    \n",
    "    model = build_model()\n",
    "    history = model.fit(partial_train_data, partial_test_data, epochs = num_epochs, batch_size = 1, verbose = 0,\n",
    "                       validation_data = (val_data, val_targets))\n",
    "    val_acc = history.history['val_acc']\n",
    "    acc = history.history['acc']\n",
    "    val_acc_history.append(val_acc)\n",
    "    acc_history.append(acc)\n",
    "\n",
    "results = model.evaluate(test_data, test_targets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_acc_history = [np.mean([x[i] for x in acc_history] ) for i in range(num_epochs)]\n",
    "avg_val_acc_history = [np.mean([x[i] for x in val_acc_history] ) for i in range(num_epochs)]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, len(avg_acc_history) + 1), avg_acc_history, 'bo', label = 'training acc')\n",
    "plt.plot(range(1, len(avg_val_acc_history) + 1), avg_val_acc_history, 'b', label = 'Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5830968677997589, 0.7, 0.6372549176216126, 0.7714285850524902]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = dict(zip([repr(d) for d in data], texts))\n",
    "test_sentences = [table[repr(a)] for a in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rights, wrongs, uncertains = [], [], []\n",
    "import math\n",
    "for i,(x,y) in enumerate(zip(test_targets, y_pred)):\n",
    "    if y[0] < 0.1 or y[0] > 0.9:\n",
    "        if x != round(y[0]):\n",
    "            wrongs.append(test_sentences[i])\n",
    "        else:\n",
    "            rights.append(test_sentences[i])\n",
    "    else:\n",
    "        uncertains.append((test_sentences[i], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendToFile(lst, filename):\n",
    "    with open(filename, \"w\") as file:\n",
    "        for l in lst:\n",
    "            file.write(str(l) + \"\\n\")\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rights) / len(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wrongs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendToFile(rights, \"./error/rights.txt\")\n",
    "appendToFile(wrongs, \"./error/wrongs.txt\")\n",
    "appendToFile(uncertains, \"./error/uncertains.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
